The following are some ongoing stuffs I tried and recorded:

Part 1:A search engine with elasticsearch and fscrawler

Setup steps and bug fixing:
1.JAVA_HOME=C:\Program Files\Java\jdk-17.0.4.1
2.cd C:\Search_Engine\fscrawler-es7-2.7
bin\fscrawler test_job
 [f.console] job [test_job] does not exist
19:55:30,528 INFO  [f.console] Do you want to create it (Y/N)?
y
19:58:21,019 INFO  [f.console] Settings have been created in [C:\Users\cheri\.fscrawler\test_job\_settings.yaml]. 
Please review and edit before relaunch

3.Create a directory named /tmp/es or c:\tmp\es, add some files you want to index in it 
(for ex. text book files DOWNLOAD from www.gutenberg.org)

4.start elasticsearch 
error:
Unrecognized VM option 'UseConcMarkSweepGC'
Error: Could not create the Java Virtual Machine.
Final solution：
open jvm.options under config subfolder：
-XX:+UseConcMarkSweepGC
-XX:CMSInitiatingOccupancyFraction=75
-XX:+UseCMSInitiatingOccupancyOnly
modify to:
#-XX:+UseConcMarkSweepGC
#-XX:CMSInitiatingOccupancyFraction=75
#-XX:+UseCMSInitiatingOccupancyOnly

save and restart elasticsearch
————————————————
5.C:\Search_Engine\fscrawler-es7-2.7>bin\fscrawler
if error occurred: Elasticsearch exception [type=master_not_discovered_exception, reason=null]
just edit elasticsearch.yaml with the following changes:
node.name: node-1
network.host: 0.0.0.0
discovery.seed_hosts: 127.0.0.1
cluster.initial_master_nodes: node-1

save the file and stop and restart elasticsearch

if running successfully:
 [f.p.e.c.f.FsCrawlerImpl] Starting FS crawler
21:10:24,005 INFO  [f.p.e.c.f.FsCrawlerImpl] FS crawler started in watch mode. It will run unless you stop it with CTRL+C.
21:10:24,649 INFO  [f.p.e.c.f.c.v.ElasticsearchClientV7] Elasticsearch Client for version 7.x connected to a node running version 7.3.2
21:10:24,903 INFO  [f.p.e.c.f.c.v.ElasticsearchClientV7] Elasticsearch Client for version 7.x connected to a node running version 7.3.2
21:10:25,727 INFO  [f.p.e.c.f.FsParserAbstract] FS crawler started for [test_job] for [\tmp\es] every [15m]
21:10:26,027 WARN  [o.a.t.p.PDFParser] J2KImageReader not loaded. JPEG2000 files will not be processed.
See https://pdfbox.apache.org/2.0/dependencies.html#jai-image-io
for optional dependencies.


#########################################################################################
Part 2:set up ElasticSearch and error fixing on windows:

Before start running “bin\elasticsearch.bat’
1.Add one line to config\elasticsearch.yml to avoid error:
ingest.geoip.downloader.enabled: false
2. change config\elasticsearch.yml
xpack.security.enabled: false  #http works, no need https (login)
3. change config\elasticsearch.yml
discovery.seed_hosts: ["127.0.0.1", "[::1]"]

ES is a distributed system
ES can be directly decompressed without configuration. One ES is decompressed on hadoop1, 
and one ES is decompressed on hadoop2, and then the two ESs are started. They form a cluster.
There is a preset configuration in ES. The default value of clustername is ElasticSearch. 
If the value is the same, it belongs to the same cluster, and different values are different clusters.

Use Management->Fleet to set up Elastic Agent:
1. config/elasticsearch.yml file configuration:
xpack.security.enabled: true
xpack.security.authc.api_key.enabled: true (add this line)
2. bin\elasticsearch-create-enrollment-token.bat --scope kibana
3. Then there will be an extra 'enterprise search' item on the home page


#####################################################################################
Part 3: Google Cloud Platform: try to install ElasticSearch/kibana on Google Kubernetes Engine
Steps:
1.$ gcloud container clusters get-credentials autopilot-cluster-1 --project=dev-poet-353407 --region=asia-east1
2.$ kubectl create -f https://download.elastic.co/downloads/eck/2.2.0/crds.yaml
3.$ kubectl apply -f https://download.elastic.co/downloads/eck/2.2.0/operator.yaml
4.$ gcloud init
5.$ helm repo add elastic https://helm.elastic.co
helm repo update
6.$cat <<EOF | kubectl apply -f -
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  nodeSets:
    - name: default
      count: 1
      config:
        node.roles: ["master", "data", "ingest", "ml", "transform"]
        xpack.ml.enabled: true
        node.store.allow_mmap: false
        path.repo: ["/tmp"]
  version: 8.1.0
EOF

$ kubectl get Elasticsearch quickstart
W0617 07:43:49.115077     975 gcp.go:120] WARNING: the gcp auth plugin is deprecated in v1.22+, unavailable in v1.25+; use gcloud instead.
To learn more, consult https://cloud.google.com/blog/products/containers-kubernetes/kubectl-auth-changes-in-gke
NAME         HEALTH   NODES   VERSION   PHASE   AGE
quickstart   green    1       8.1.0     Ready   11m

7.$ cat <<EOF | kubectl apply -f -
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: quickstart
spec:
  version: 8.1.0
  count: 1
  elasticsearchRef:
    name: "quickstart"  
EOF	
8.$ kubectl get services
W0617 07:50:12.830403     994 gcp.go:120] WARNING: the gcp auth plugin is deprecated in v1.22+, unavailable in v1.25+; use gcloud instead.
To learn more, consult https://cloud.google.com/blog/products/containers-kubernetes/kubectl-auth-changes-in-gke
NAME                          TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
kubernetes                    ClusterIP   10.54.0.1     <none>        443/TCP    2d
quickstart-es-default         ClusterIP   None          <none>        9200/TCP   18m
quickstart-es-http            ClusterIP   10.54.1.4     <none>        9200/TCP   18m
quickstart-es-internal-http   ClusterIP   10.54.3.110   <none>        9200/TCP   18m
quickstart-es-transport       ClusterIP   None          <none>        9300/TCP   18m
quickstart-kb-http            ClusterIP   10.54.0.2     <none>        5601/TCP   18m

9.$ kubectl port-forward svc/quickstart-es-http 9200:9200

10.$ curl https://localhost:9200 -k -u elastic:N7w3tr762Q8WTukuJY653K6p
